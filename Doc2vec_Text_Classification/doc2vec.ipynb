{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#For converting all the words to stem words in lowercase\n",
    "snowball_stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "punctRemover=str.maketrans('','',string.punctuation)\n",
    "\n",
    "X = [] #To store actual text of the author\n",
    "Y = [] #To store author name\n",
    "\n",
    "innerStr = \" \"\n",
    "strToAppend= \" \"\n",
    "\n",
    "austen = gutenberg.sents('austen-emma.txt') # can choose another author name from the available choices\n",
    "chester = gutenberg.sents('chesterton-thursday.txt')\n",
    "\n",
    "#To remove the punctuation\n",
    "punctRemover=str.maketrans('','',string.punctuation)\n",
    "\n",
    "for row in austen:\n",
    "    if row:\n",
    "        innerStr = ' '.join([str(elem) for elem in row]) #convert the list to single string\n",
    "        X.append(innerStr.translate(punctRemover))\n",
    "        Y.append('austen')\n",
    "        \n",
    "for row in chester:\n",
    "    if row:\n",
    "        innerStr = ' '.join([str(elem) for elem in row]) #convert the list to single string\n",
    "        X.append(innerStr.translate(punctRemover))\n",
    "        Y.append('chester') \n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size = 0.2)#split into training and test set\n",
    "Y_train, Y_test = train_test_split(Y, test_size = 0.2)# to make sure the label size remains same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9195/9195 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import multiprocessing\n",
    "from gensim.models import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "from sklearn import svm\n",
    "\n",
    "train_tagged = []\n",
    "test_tagged = []\n",
    "\n",
    "for i in range (0, len(X_train)):\n",
    "    train_tagged.append(gensim.models.doc2vec.TaggedDocument(words=gensim.utils.simple_preprocess(X_train[i]), tags=[Y_train[i]]))\n",
    "\n",
    "for i in range (0, len(X_test)):\n",
    "    test_tagged.append(gensim.models.doc2vec.TaggedDocument(words=gensim.utils.simple_preprocess(X_test[i]), tags=[Y_test[i]]))\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# dm = 1 will use distributed memory model and it will preserves the word order in a document\n",
    "doc2vec_model = Doc2Vec(dm=1, vector_size=300, min_count=2, workers=cores)\n",
    "doc2vec_model.build_vocab([x for x in tqdm(train_tagged)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    doc2vec_model.train(utils.shuffle([x for x in (train_tagged)]), total_examples=len(train_tagged), epochs=1)\n",
    "    doc2vec_model.alpha -= 0.002\n",
    "    doc2vec_model.min_alpha = doc2vec_model.alpha\n",
    "    \n",
    "# Building the feature vector for the classifier\n",
    "def feature_vector(model, docs):\n",
    "    doc2vec_vectors = [model.infer_vector(doc.words) for doc in docs]\n",
    "    targets = [doc.tags[0] for doc in docs]\n",
    "    return doc2vec_vectors, targets\n",
    "\n",
    "# Translating docs into vectors for training and test set\n",
    "X_doc2vec_train, y_doc2vec_train = feature_vector(doc2vec_model, train_tagged)\n",
    "X_doc2vec_test, y_doc2vec_test = feature_vector(doc2vec_model, test_tagged)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
       "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a classification model\n",
    "\n",
    "#C: 1e5, Since we can not visualize a linear relationalship in 300 dimensional vector to the target, We take small value of C\n",
    "#gamma:'1'. We have trained the model on 300-dimension of the document vector \n",
    "#           If it is chosen too small, the area of influence of any point will increase.\n",
    "log_doc2vec = svm.SVC(C=0.1, gamma = 1)\n",
    "log_doc2vec.fit(X_doc2vec_train, y_doc2vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from doc2vec:  68.37755545889517 %\n",
      "\n",
      "Confusion Matrix for doc2vec model:\n",
      "[[1572    0]\n",
      " [ 727    0]]\n",
      "\n",
      "Doc2vec logistic classifier:\n",
      " Testing accuracy 0.6837755545889517\n",
      " Testing F1 score: 0.5553578774548343\n"
     ]
    }
   ],
   "source": [
    "#doc2vec predictor Accuracy and confusion matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "y_doc2vecPred = log_doc2vec.predict(X_doc2vec_test)\n",
    "print('Accuracy from doc2vec: ', 100* np.mean(y_doc2vecPred == y_doc2vec_test), \"%\")\n",
    "\n",
    "print('\\nConfusion Matrix for doc2vec model:')\n",
    "results = confusion_matrix(y_doc2vec_test, y_doc2vecPred) \n",
    "print(results) \n",
    "\n",
    "print('\\nDoc2vec logistic classifier:')\n",
    "print(' Testing accuracy %s' % accuracy_score(Y_test, y_doc2vecPred))\n",
    "print(' Testing F1 score: {}'.format(f1_score(Y_test, y_doc2vecPred, average='weighted')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
